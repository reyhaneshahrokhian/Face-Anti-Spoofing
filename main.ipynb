{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-5xmWxstuQxW",
        "g9SvlTEOtpMq",
        "jkvQMv1ttSvq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IDsPX8SMwKPL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.fft import fft2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4SJNQHd6UiZ",
        "outputId": "dde22e18-66f1-43f3-c0a6-9975216fff7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "-5xmWxstuQxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "def extract_frames(video_path, folder_name, output_dir):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    middle_frame = length // 2\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "      frame_name = f\"{folder_name}_{video_name}_frame.jpg\"\n",
        "      frame_path = os.path.join(output_dir, frame_name)\n",
        "      cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frame_path\n",
        "\n",
        "def preprocess_dataset(data_dir, output_dir, label_file):\n",
        "    all_frame_paths = []\n",
        "    all_labels = []\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for folder in os.listdir(data_dir):\n",
        "        folder_path = os.path.join(data_dir, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            for video in os.listdir(folder_path):\n",
        "                video_path = os.path.join(folder_path, video)\n",
        "                if video.endswith('.avi'):\n",
        "                    if video in ['1.avi', '2.avi', 'HR_1.avi', 'HR_4.avi']:\n",
        "                        label = 1  # live\n",
        "                    else:\n",
        "                        label = 0  # spoofed\n",
        "                    frame = extract_frames(video_path, folder, output_dir)\n",
        "                    all_frame_paths.append(frame)\n",
        "                    all_labels.append(label)\n",
        "\n",
        "    # Save labels to a CSV file\n",
        "    label_df = pd.DataFrame({\n",
        "        'frame_path': all_frame_paths,\n",
        "        'label': all_labels\n",
        "    })\n",
        "    label_df.to_csv(label_file, index=False)\n"
      ],
      "metadata": {
        "id": "1GhLe5y2w8oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/train_release'\n",
        "output_dir = '/content/drive/My Drive/frames'\n",
        "label_file = '/content/drive/My Drive/labels.csv'\n",
        "\n",
        "preprocess_dataset(data_dir, output_dir, label_file)\n",
        "print(\"Preprocessing complete. Frames and labels are saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4rHgk846cpD",
        "outputId": "16f63c99-1e96-4ca6-8e67-1e71d47b7f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Frames and labels are saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature extraction model"
      ],
      "metadata": {
        "id": "0locq7watrva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.feature import local_binary_pattern\n",
        "from numpy.fft import fft2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from sklearn import svm\n",
        "import joblib\n",
        "\n",
        "def extract_hsv_histograms(img):\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(img_hsv)\n",
        "\n",
        "    return s.flatten()\n",
        "\n",
        "def extract_lbp_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
        "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
        "    hist = hist.astype(\"float\")\n",
        "    hist /= (hist.sum() + 1e-6)\n",
        "    return hist\n",
        "\n",
        "def extract_frequency_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    f_transform = fft2(gray)\n",
        "    f_transform = np.abs(f_transform)\n",
        "    return f_transform.flatten()\n",
        "\n",
        "def load_and_extract_features_in_batches(label_file, batch_size=32, target_size=(224, 224)):\n",
        "    data = pd.read_csv(label_file)\n",
        "    features = []\n",
        "    labels = []\n",
        "    filenames = []\n",
        "    total_samples = len(data)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        batch_data = data.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
        "        for _, row in batch_data.iterrows():\n",
        "            frame_path = row['frame_path']\n",
        "            label = row['label']\n",
        "            try:\n",
        "                frame = cv2.imread(frame_path)\n",
        "                if frame is None:\n",
        "                    print(f\"Warning: Could not read frame {frame_path}\")\n",
        "                    continue\n",
        "                frame = cv2.resize(frame, target_size)\n",
        "\n",
        "                hist_features = extract_hsv_histograms(frame)\n",
        "                lbp_features = extract_lbp_features(frame)\n",
        "                #freq_features = extract_frequency_features(frame)\n",
        "                combined_features = np.concatenate((hist_features, lbp_features))\n",
        "                features.append(combined_features)\n",
        "                labels.append(label)\n",
        "                filenames.append(frame_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {frame_path}: {e}\")\n",
        "    return np.array(features), np.array(labels), filenames\n",
        "\n",
        "# Load and extract features in batches\n",
        "label_file = '/content/drive/My Drive/labels.csv'\n",
        "X_features, y_features, filenames = load_and_extract_features_in_batches(label_file)\n",
        "\n",
        "# Check if features were extracted successfully\n",
        "if len(X_features) == 0 or len(y_features) == 0:\n",
        "    raise ValueError(\"No features extracted. Check the data and preprocessing steps.\")\n",
        "\n",
        "# Split the data\n",
        "X_train_feat, X_test_feat, y_train_feat, y_test_feat, train_filenames, test_filenames = train_test_split(\n",
        "    X_features, y_features, filenames, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train_feat, y_train_feat)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(clf, '/content/drive/My Drive/svm.joblib')\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_feat = clf.predict(X_test_feat)\n",
        "print(\"Feature extraction method accuracy:\", accuracy_score(y_test_feat, y_pred_feat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5h6KLWfRuW-",
        "outputId": "0d41376d-835a-4931-ba72-48a9498226ef"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction method accuracy: 0.9083333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "import joblib\n",
        "clf_model = joblib.load('/content/drive/My Drive/svm.joblib')"
      ],
      "metadata": {
        "id": "LCTQCkmgeqHM"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep model"
      ],
      "metadata": {
        "id": "g9SvlTEOtpMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
        "from tensorflow.keras.applications import MobileNetV2, VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Build MobileFaceNet model\n",
        "def build_mobilefacenet_model(input_shape):\n",
        "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Data generator\n",
        "def data_generator_for_dl(data, batch_size=32, target_size=(224, 224)):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    while True:\n",
        "        for start in range(0, len(data), batch_size):\n",
        "            end = min(start + batch_size, len(data))\n",
        "            batch_data = data.iloc[start:end]\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            for _, row in batch_data.iterrows():\n",
        "                frame_path = row['frame_path']\n",
        "                label = row['label']\n",
        "                image = cv2.imread(frame_path)\n",
        "                if image is not None:\n",
        "                    image = cv2.resize(image, target_size)\n",
        "                    image = image / 255.0\n",
        "                    batch_images.append(image)\n",
        "                    batch_labels.append(label)\n",
        "            yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "# Load and split the data\n",
        "label_file = '/content/drive/My Drive/labels.csv'\n",
        "#train_data, test_data = train_test_split(pd.read_csv(label_file), test_size=0.2, random_state=42)\n",
        "train_data = pd.read_csv(label_file)\n",
        "\n",
        "# Generators for training and validation\n",
        "batch_size = 32\n",
        "train_gen = data_generator_for_dl(train_data, batch_size=batch_size)\n",
        "#val_gen = data_generator_for_dl(test_data, batch_size=batch_size)\n",
        "\n",
        "# Build model\n",
        "input_shape = (224, 224, 3)\n",
        "mobilefacenet_model = build_mobilefacenet_model(input_shape)\n",
        "\n",
        "# Train models\n",
        "epochs = 7\n",
        "steps_per_epoch = len(train_data) // batch_size\n",
        "#validation_steps = len(test_data) // batch_size\n",
        "\n",
        "mobilefacenet_model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=epochs)#, validation_data=val_gen, validation_steps=validation_steps)\n",
        "\n",
        "# Save model\n",
        "mobilefacenet_model.save('/content/drive/My Drive/mobilefacenet_model2.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "# def evaluate_model(model, data, batch_size=32, target_size=(224, 224)):\n",
        "#     gen = data_generator_for_dl(data, batch_size, target_size)\n",
        "#     steps = len(data) // batch_size\n",
        "#     return model.evaluate(gen, steps=steps)\n",
        "\n",
        "# mobilefacenet_acc = evaluate_model(mobilefacenet_model, test_data)\n",
        "\n",
        "# print(\"MobileFaceNet Test Accuracy:\", mobilefacenet_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVfTSuVLSV9w",
        "outputId": "b70c1655-910e-4233-c69c-578c1ca1f3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "18/18 [==============================] - 12s 407ms/step - loss: 0.2676 - accuracy: 0.8819\n",
            "Epoch 2/7\n",
            "18/18 [==============================] - 7s 334ms/step - loss: 0.0547 - accuracy: 0.9824\n",
            "Epoch 3/7\n",
            "18/18 [==============================] - 8s 431ms/step - loss: 0.0334 - accuracy: 0.9912\n",
            "Epoch 4/7\n",
            "18/18 [==============================] - 7s 378ms/step - loss: 0.0253 - accuracy: 0.9982\n",
            "Epoch 5/7\n",
            "18/18 [==============================] - 8s 436ms/step - loss: 0.0206 - accuracy: 0.9982\n",
            "Epoch 6/7\n",
            "18/18 [==============================] - 7s 383ms/step - loss: 0.0199 - accuracy: 0.9965\n",
            "Epoch 7/7\n",
            "18/18 [==============================] - 8s 437ms/step - loss: 0.0204 - accuracy: 0.9982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a model from file\n",
        "mobilefacenet_model = tf.keras.models.load_model('/content/drive/My Drive/mobilefacenet_model.h5')"
      ],
      "metadata": {
        "id": "3hY_2MwTf_RZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test deep"
      ],
      "metadata": {
        "id": "jkvQMv1ttSvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        video_paths = [line.strip() for line in file]\n",
        "    return video_paths"
      ],
      "metadata": {
        "id": "EgAHLozstnwh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the middle frame of a video\n",
        "def extract_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    middle_frame = length // 2\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "    if ret:\n",
        "        return frame\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = preprocess_input(frame)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "VDCjB2nf3G0G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "def calculate_liveness_score(frame):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)\n",
        "    preds = mobilefacenet_model.predict(preprocessed_frame)\n",
        "    # Assuming the liveness score can be inferred from the prediction\n",
        "    liveness_score = np.max(preds)\n",
        "    return liveness_score"
      ],
      "metadata": {
        "id": "2lmEYABN3Pjb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install face_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8beWdgsn-fG",
        "outputId": "4ad5f4d1-4e4f-4bfa-edd6-b366fe4170c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=3f5eb215bed4b89ef8d3b8361cce28bee154c51f699262f9e3f4b70ec408f85b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import face_recognition\n",
        "\n",
        "\n",
        "def recognize_face(frame,file_name):\n",
        "    # Convert the frame to RGB (OpenCV uses BGR by default)\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Find all face locations and face encodings in the frame\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "\n",
        "    for i, face_location in enumerate(face_locations):\n",
        "        # Extract the face location\n",
        "        top, right, bottom, left = face_location\n",
        "\n",
        "        # Add a margin to the face location\n",
        "        # margin = 30\n",
        "        # top = max(0, top - margin)\n",
        "        # right = min(frame.shape[1], right + margin)\n",
        "        # bottom = min(frame.shape[0], bottom + margin)\n",
        "        # left = max(0, left - margin)\n",
        "        top = max(0, top - 250)\n",
        "        right = min(frame.shape[1], right + 40)\n",
        "        bottom = min(frame.shape[0], bottom + 40)\n",
        "        left = max(0, left - 40)\n",
        "        # Crop the face from the frame\n",
        "        face_image = frame[top:bottom, left:right]\n",
        "        # print(file_name)\n",
        "        full_path = f'/content/recog/{file_name}.jpg'\n",
        "        #print(full_path)\n",
        "        cv2.imwrite(full_path, face_image)\n",
        "        return face_image"
      ],
      "metadata": {
        "id": "h8w75OnymkOh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_frequency_features2(image, target_size=(224, 224)):\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply 2D Fourier Transform\n",
        "    dft = cv2.dft(np.float32(gray_image), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
        "\n",
        "    # Shift the zero-frequency component\n",
        "    dft_shift = np.fft.fftshift(dft)\n",
        "\n",
        "    # Calculate the magnitude spectrum\n",
        "    magnitude_spectrum = 20 * np.log(cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1]))\n",
        "\n",
        "    # Resize the magnitude spectrum\n",
        "    resized_magnitude_spectrum = cv2.resize(magnitude_spectrum, target_size)\n",
        "\n",
        "    # Create a 3-channel image from the magnitude spectrum\n",
        "    frequency_features = np.stack((resized_magnitude_spectrum, resized_magnitude_spectrum, resized_magnitude_spectrum), axis=-1)\n",
        "\n",
        "    return frequency_features"
      ],
      "metadata": {
        "id": "FiuXC3lpnL2T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "video_paths = read_dataset('/content/drive/My Drive/dataset.txt')\n",
        "result = pd.DataFrame(columns=['filename','liveness_score','liveness_score_crop','liveness_score_frequency'])\n",
        "for video_path in video_paths:\n",
        "  parts = video_path.split('/')\n",
        "  filename = parts[-1].split('.')[0]\n",
        "  frame = extract_frame(video_path)\n",
        "  if frame is not None:\n",
        "    liveness_score = calculate_liveness_score(frame)\n",
        "    face = recognize_face(frame,filename)\n",
        "    if face is not None:\n",
        "      liveness_score_crop = calculate_liveness_score(face)\n",
        "    else:\n",
        "      liveness_score_crop = 0\n",
        "    frequency_features = extract_frequency_features2(frame)\n",
        "    if frequency_features is not None:\n",
        "      liveness_score_frequency = calculate_liveness_score(frequency_features)\n",
        "    else:\n",
        "      liveness_score_frequency = 0\n",
        "\n",
        "    row = pd.DataFrame([{'filename': filename, 'liveness_score': liveness_score, 'liveness_score_crop': liveness_score_crop, 'liveness_score_frequency': liveness_score_frequency}])\n",
        "    result = pd.concat([result, row], ignore_index=True)"
      ],
      "metadata": {
        "id": "gyOe20Fg3inU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3e257c-61c5-43ba-eee0-332a967c097b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('/content/drive/My Drive/prediction_deep.csv', index=False)"
      ],
      "metadata": {
        "id": "p4d_xkutm9S8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test feature"
      ],
      "metadata": {
        "id": "ItQzBIJivS_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from skimage.feature import local_binary_pattern\n",
        "from numpy.fft import fft2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# Function to preprocess frame for RandomForest model\n",
        "def preprocess_frame_rf(frame):\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    hist_features = extract_hsv_histograms(frame)\n",
        "    lbp_features = extract_lbp_features(frame)\n",
        "    #freq_features = extract_frequency_features(frame)\n",
        "    combined_features = np.concatenate((hist_features, lbp_features))\n",
        "    return combined_features.reshape(1, -1)\n",
        "\n",
        "# Function to calculate liveness score\n",
        "def calculate_liveness_score2(frame):\n",
        "    frame = preprocess_frame_rf(frame)\n",
        "    preds = clf_model.predict(frame)\n",
        "    liveness_score = preds[0]\n",
        "    return liveness_score\n",
        "\n",
        "# Read video paths\n",
        "video_paths = read_dataset('/content/drive/My Drive/dataset.txt')\n",
        "\n",
        "result = pd.DataFrame(columns=['filename','liveness_score','liveness_score_crop','liveness_score_frequency'])\n",
        "\n",
        "for video_path in video_paths:\n",
        "    parts = video_path.split('/')\n",
        "    filename = parts[-1].split('.')[0]\n",
        "    frame = extract_frame(video_path)\n",
        "    if frame is not None:\n",
        "        face = recognize_face(frame, filename)\n",
        "        frequency_features = extract_frequency_features2(frame)\n",
        "\n",
        "        liveness_score = calculate_liveness_score2(frame)\n",
        "        if face is not None:\n",
        "            liveness_score_crop = calculate_liveness_score2(face)\n",
        "        else:\n",
        "            liveness_score_crop = 0\n",
        "        if frequency_features is not None:\n",
        "            liveness_score_frequency = calculate_liveness_score2(frequency_features)\n",
        "        else:\n",
        "            liveness_score_frequency = 0\n",
        "\n",
        "        row = pd.DataFrame([{\n",
        "            'filename': filename,\n",
        "            'liveness_score': liveness_score,\n",
        "            'liveness_score_crop': liveness_score_crop,\n",
        "            'liveness_score_frequency': liveness_score_frequency,\n",
        "        }])\n",
        "        result = pd.concat([result, row], ignore_index=True)"
      ],
      "metadata": {
        "id": "szkquLXpvnXt"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqqm1ro3YrTo",
        "outputId": "29babc0f-c1ec-457b-e787-efe6d016de25"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       filename liveness_score liveness_score_crop liveness_score_frequency\n",
            "0   anti-spoof1              1                   1                        1\n",
            "1   anti-spoof2              0                   1                        1\n",
            "2   anti-spoof3              0                   0                        1\n",
            "3   anti-spoof4              0                   0                        1\n",
            "4   anti-spoof5              0                   0                        1\n",
            "5   anti-spoof6              1                   1                        1\n",
            "6   anti-spoof7              1                   1                        1\n",
            "7   anti-spoof8              1                   0                        1\n",
            "8   anti-spoof9              1                   1                        1\n",
            "9        spoof1              0                   1                        1\n",
            "10       spoof2              0                   0                        1\n",
            "11       spoof3              0                   0                        1\n",
            "12       spoof4              0                   1                        1\n",
            "13       spoof5              1                   1                        1\n",
            "14       spoof6              0                   0                        1\n",
            "15       spoof7              0                   1                        1\n",
            "16       spoof8              0                   0                        1\n",
            "17       spoof9              1                   1                        1\n",
            "18      spoof10              1                   0                        1\n",
            "19      spoof11              0                   0                        1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('/content/drive/My Drive/prediction_feature.csv', index=False)"
      ],
      "metadata": {
        "id": "nlElWx6YxnlZ"
      },
      "execution_count": 115,
      "outputs": []
    }
  ]
}